{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB0H5js0ddKr"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyJmyT22qR8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e0a756-75b7-4bf4-c70e-42f516509a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o8rJ8F5AxSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd52bf60-ea44-4bf6-c18e-ebe68cd01644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 13 09:55:20 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0    32W /  70W |   7946MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg27CdYOdfyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ae1b17-86c7-4745-c4bb-4cce1afd6db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 73.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade transformers\n",
        "# ! pip3 install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -V"
      ],
      "metadata": {
        "id": "J35-BufHxX77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade torchmetrics"
      ],
      "metadata": {
        "id": "xVDtlsbMq9g5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b2b6a0-4001-488d-c565-4fd904d428d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 41.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 332 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT0nlmIGc9DP"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgfNMwnFcgpT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils import rnn\n",
        "from transformers import BertTokenizer\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# For Configuration\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# For Model\n",
        "from transformers import AutoTokenizer, AutoModelForPreTraining, AutoModel\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
        "\n",
        "# For Train and Valid and Test\n",
        "from tqdm.auto import tqdm\n",
        "from torchmetrics import SpearmanCorrcoef\n",
        "\n",
        "# To control which GPU to run on\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOn_NMmTfMUb"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "Users only have to adjust the parameters here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E05bB3ffPXE"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BaseCFG:\n",
        "    batch_size: int\n",
        "    downstream_lr: float \n",
        "    audio_encoder_lr: float \n",
        "    weight_decay : float \n",
        "    audio_encoder_model : str \n",
        "    audio_embedding : int \n",
        "    hidden_dim : int \n",
        "    intent_dim : int \n",
        "    \n",
        "    dataset : str\n",
        "    task_type : str\n",
        "    # Is it correct?\n",
        "    max_length : int\n",
        "    data_root : str\n",
        "\n",
        "    # For Asrglue\n",
        "    subtask : str = \"\"\n",
        "    noise_level: str = \"\"\n",
        "    \n",
        "    trainable : bool = True\n",
        "    num_of_workers : int = 2\n",
        "\n",
        "    project_root : str = \"/content/drive/Shareddrives/miulab/checkpoints/\"\n",
        "    patience : int = 1\n",
        "    factor : float = 0.8\n",
        "\n",
        "    device : torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    reduction : str = \"mean\"\n",
        "    text_tokenizer : str = \"distilbert-base-uncased\"\n",
        "    epochs : int = 30\n",
        "    checkpoint_continue : bool = False\n",
        "\n",
        "    # for random split ASRGLUE dataset\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "@dataclass\n",
        "class FluentSpeechCFG(BaseCFG):\n",
        "    dataset = \"fsc\"\n",
        "    task_type = \"classification\"\n",
        "    max_length = 128\n",
        "    data_root = \"/content/drive/Shareddrives/miulab/fluent_speech_commands_dataset/\"\n",
        "    intent_dim = 31\n",
        "\n",
        "@dataclass\n",
        "class AsrglueCFG(BaseCFG):\n",
        "    dataset = \"asrglue\"\n",
        "    max_length = 512 # useless\n",
        "    data_root = \"/content/drive/Shareddrives/miulab/ASRGLUE/dev/\"\n",
        " \n",
        "@dataclass\n",
        "class AsrglueStsbCFG(AsrglueCFG):\n",
        "    task_type = \"prediction\"\n",
        "    subtask = \"sts-b\"\n",
        "    intent_dim = 1\n",
        "    noise_level = \"low\"\n",
        "    downstream_lr = 1e-5\n",
        "\n",
        "@dataclass\n",
        "class AsrglueSst2CFG(AsrglueCFG):\n",
        "    task_type = \"classification\"\n",
        "    subtask : str = \"sst-2\"\n",
        "    intent_dim = 2\n",
        "    noise_level = \"low\"\n",
        "    downstream_lr = 1e-3\n",
        "\n",
        "@dataclass\n",
        "class AsrglueRteCFG(AsrglueCFG):\n",
        "    task_type = \"classification\"\n",
        "    subtask : str = \"rte\"\n",
        "    intent_dim = 2\n",
        "    noise_level = \"low\"\n",
        "    downstream_lr = 1e-5\n",
        "\n",
        "@dataclass\n",
        "class AsrglueQqpCFG(AsrglueCFG):\n",
        "    task_type = \"classification\"\n",
        "    subtask : str = \"qqp\"\n",
        "    intent_dim = 2\n",
        "    noise_level = \"low\"\n",
        "    downstream_lr = 1e-4\n",
        "\n",
        "@dataclass\n",
        "class AsrglueQnliCFG(AsrglueCFG):\n",
        "    task_type = \"classification\"\n",
        "    subtask : str = \"qnli\"\n",
        "    intent_dim = 2\n",
        "    noise_level = \"low\"\n",
        "    downstream_lr = 1e-5\n",
        "\n",
        "@dataclass\n",
        "class AsrglueScitailCFG(AsrglueCFG):\n",
        "    task_type = \"classification\"\n",
        "    subtask : str = \"scitail\"\n",
        "    intent_dim = 2\n",
        "    noise_level = \"low\"\n",
        "    downstream_lr = 1e-5\n",
        "\n",
        "@dataclass\n",
        "class HubertAsrglueStsbCFG(AsrglueStsbCFG):\n",
        "    batch_size = 1\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/hubert-base-ls960'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"hubert_asrglue_stsb_test8_\"\n",
        "\n",
        "@dataclass\n",
        "class Wav2Vec2AsrglueStsbCFG(AsrglueStsbCFG):\n",
        "    batch_size = 2\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/wav2vec2-base-960h'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"wav2vec2_asrglue_stsb_\"\n",
        "\n",
        "@dataclass\n",
        "class HubertAsrglueSst2CFG(AsrglueSst2CFG):\n",
        "    batch_size = 2\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/hubert-base-ls960'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"hubert_asrglue_sst2_\"\n",
        "\n",
        "@dataclass\n",
        "class Wav2Vec2AsrglueSst2CFG(AsrglueSst2CFG):\n",
        "    batch_size = 1\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/wav2vec2-base-960h'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"wav2vec2_asrglue_sst2_\"\n",
        "\n",
        "@dataclass\n",
        "class HubertAsrglueRteCFG(AsrglueRteCFG):\n",
        "    batch_size = 1\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/hubert-base-ls960'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"hubert_asrglue_rte_\"\n",
        "\n",
        "@dataclass\n",
        "class Wav2Vec2AsrglueRteCFG(AsrglueRteCFG):\n",
        "    batch_size = 2\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/wav2vec2-base-960h'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"wav2vec2_asrglue_rte_\"\n",
        "\n",
        "@dataclass\n",
        "class HubertAsrglueQqpCFG(AsrglueQqpCFG):\n",
        "    batch_size = 1\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/hubert-base-ls960'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"hubert_asrglue_qqp_\"\n",
        "\n",
        "@dataclass\n",
        "class Wav2Vec2AsrglueQqpCFG(AsrglueQqpCFG):\n",
        "    batch_size = 2\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/wav2vec2-base-960h'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"wav2vec2_asrglue_qqp_\"\n",
        "\n",
        "@dataclass\n",
        "class HubertAsrglueQnliCFG(AsrglueQnliCFG):\n",
        "    batch_size = 1\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/hubert-base-ls960'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"hubert_asrglue_qnli_\"\n",
        "\n",
        "@dataclass\n",
        "class Wav2Vec2AsrglueQnliCFG(AsrglueQnliCFG):\n",
        "    batch_size = 2\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/wav2vec2-base-960h'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"wav2vec2_asrglue_qnli_\"\n",
        "\n",
        "@dataclass\n",
        "class HubertAsrglueScitailCFG(AsrglueScitailCFG):\n",
        "    batch_size = 2\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/hubert-base-ls960'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"hubert_asrglue_scitail_\"\n",
        "\n",
        "@dataclass\n",
        "class Wav2Vec2AsrglueScitailCFG(AsrglueScitailCFG):\n",
        "    batch_size = 2\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/wav2vec2-base-960h'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"wav2vec2_asrglue_scitail_\"\n",
        "\n",
        "@dataclass\n",
        "class HubertFscCFG(FluentSpeechCFG): \n",
        "    batch_size = 4\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/hubert-base-ls960'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"hubert_fsc_\"\n",
        "\n",
        "@dataclass\n",
        "class Wav2Vec2FscCFG(FluentSpeechCFG):\n",
        "    batch_size = 4\n",
        "    downstream_lr = 1e-4\n",
        "    audio_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-2\n",
        "    audio_encoder_model = 'facebook/wav2vec2-base-960h'\n",
        "    audio_embedding = 768\n",
        "    hidden_dim = 256\n",
        "    project_root = BaseCFG.project_root + \"wav2vec2_fsc_\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# curCFG = HubertAsrglueStsbCFG\n",
        "curCFG = Wav2Vec2AsrglueScitailCFG\n",
        "do_train = True\n",
        "do_test = True"
      ],
      "metadata": {
        "id": "fSI_caf-KAml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAKidzX6tRmu"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTcgg_5UtYHY"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum_val = 0.0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, values):\n",
        "        self.sum_val += np.sum(values)\n",
        "        self.count += len(values)\n",
        "\n",
        "    def get(self):\n",
        "        return self.sum_val / self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1WB_wr8dLqE"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJECxjK-dRNe"
      },
      "source": [
        "## BaseDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR-WMbUGdNR1"
      },
      "outputs": [],
      "source": [
        "class BaseDataset(Dataset):\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def labels_list(self):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t_Fs9_ue6Cq"
      },
      "source": [
        "## FSC Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0la0NXgqunf"
      },
      "outputs": [],
      "source": [
        "class FluentSpeechCommandsDataset(BaseDataset):\n",
        "\n",
        "    def __init__(self, data_root, split='train', intent_encoder=None):\n",
        "        assert split in ['train', 'test', 'valid'], 'Invalid split'\n",
        "        print(f\"init {split} dataset....\")\n",
        "\n",
        "        self.CFG = FluentSpeechCFG\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.df = pd.read_csv(os.path.join(self.data_root, 'data/', '{}_data.csv'.format(split)))\n",
        "        self.df['intent'] = self.df[['action', 'object', 'location']].apply('-'.join, axis=1)\n",
        "\n",
        "        if intent_encoder is None:\n",
        "            intent_encoder = preprocessing.LabelEncoder()\n",
        "            intent_encoder.fit(self.df['intent'])\n",
        "        self.intent_encoder = intent_encoder\n",
        "        self.df['intent_label'] = intent_encoder.transform(self.df['intent'])\n",
        "\n",
        "        self.labels_set = set(self.df['intent_label'])\n",
        "        self.label2idx = {}\n",
        "        for label in self.labels_set:\n",
        "            idx = np.where(self.df['intent_label'] == label)[0]\n",
        "            self.label2idx[label] = idx\n",
        "\n",
        "        self.distilbert_tokenizer = DistilBertTokenizer.from_pretrained(self.CFG.text_tokenizer)\n",
        "\n",
        "    def load_audio(self, idx):\n",
        "        df_row = self.df.iloc[idx]\n",
        "        filename = os.path.join(self.data_root, df_row['path'])\n",
        "        waveform, sample_rate = torchaudio.load(filename)\n",
        "        intent = df_row['intent_label']\n",
        "        encoding = self.distilbert_tokenizer(\n",
        "            df_row['transcription'],\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.CFG.max_length\n",
        "        )\n",
        "        return waveform.squeeze(), intent, encoding, df_row['transcription']\n",
        "\n",
        "    def get_dict(self, waveform, intent, encoding, transcription, suffix=''):\n",
        "        ret_dict = {\n",
        "            'waveform':waveform,\n",
        "            'label':intent,\n",
        "            'encoded_text':torch.tensor(encoding['input_ids']).flatten(),\n",
        "            'text_length':torch.tensor(encoding['input_ids']).flatten().shape[0],\n",
        "            'raw_text':transcription\n",
        "        }\n",
        "        ret_dict = {k+suffix:v for k,v in ret_dict.items()}\n",
        "        return ret_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform, intent, encoding, transcription = self.load_audio(idx)\n",
        "        ret = self.get_dict(waveform, intent, encoding, transcription)\n",
        "        return ret\n",
        "\n",
        "    def labels_list(self):\n",
        "        return self.intent_encoder.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ASRGLUE Dataset"
      ],
      "metadata": {
        "id": "xz573zTvJhsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ASRGLUEDataset(BaseDataset):\n",
        "\n",
        "    def __init__(self, CFG, split='train', intent_encoder=None):\n",
        "        assert split in ['train', 'test', 'valid'], 'Invalid split'\n",
        "\n",
        "        self.CFG = CFG\n",
        "\n",
        "        self.data_root = CFG.data_root\n",
        "        self.df = pd.read_csv(os.path.join(self.data_root, '{}_{}_{}.csv'.format(self.CFG.subtask, self.CFG.noise_level, split)))\n",
        "\n",
        "        if self.CFG.task_type == 'classification' :\n",
        "            if intent_encoder is None:\n",
        "                intent_encoder = preprocessing.LabelEncoder()\n",
        "                intent_encoder.fit(self.df['label'])\n",
        "            self.intent_encoder = intent_encoder\n",
        "            self.df['label'] = intent_encoder.transform(self.df['label'])\n",
        "\n",
        "            self.labels_set = set(self.df['label'])\n",
        "            self.label2idx = {}\n",
        "            for label in self.labels_set:\n",
        "                idx = np.where(self.df['label'] == label)[0]\n",
        "                self.label2idx[label] = idx\n",
        "\n",
        "    def load_audio(self, idx):\n",
        "        resample_rate = 16000\n",
        "        df_row = self.df.iloc[idx]\n",
        "        filename1 = os.path.join(self.data_root, self.CFG.subtask, df_row['path'])\n",
        "        waveform1, sample_rate1 = torchaudio.load(filename1)\n",
        "        if sample_rate1 != resample_rate:\n",
        "            # print(f\"sample rate1 {sample_rate1}\")\n",
        "            resampler = T.Resample(sample_rate1, resample_rate)\n",
        "            resampled_waveform = resampler(waveform1)\n",
        "            waveform1 = resampled_waveform\n",
        "        waveform1 = waveform1.squeeze()\n",
        "        label = df_row['label']\n",
        "\n",
        "        if self.CFG.subtask in ['sts-b', 'rte', 'qqp', 'qnli', 'scitail'] :\n",
        "            filename2 = os.path.join(self.data_root, self.CFG.subtask, df_row['path2'])\n",
        "            waveform2, sample_rate2 = torchaudio.load(filename2)\n",
        "            if sample_rate2 != resample_rate:\n",
        "                # print(f\"sample rate2 {sample_rate2}\")\n",
        "                resampler = T.Resample(sample_rate2, resample_rate)\n",
        "                resampled_waveform = resampler(waveform1)\n",
        "                waveform2 = resampled_waveform\n",
        "            waveform2 = waveform2.squeeze()\n",
        "            waveform = torch.tensor(np.concatenate([waveform1, np.zeros(10000), waveform2]))\n",
        "        else :\n",
        "            waveform = waveform1\n",
        "\n",
        "        return waveform, df_row['label']\n",
        "\n",
        "    def get_dict(self, waveform, label, suffix=''):\n",
        "        ret_dict = {\n",
        "            'waveform':waveform,\n",
        "            'label':label,\n",
        "        }\n",
        "        ret_dict = {k+suffix:v for k,v in ret_dict.items()}\n",
        "        # print(f\"in get dict waveform len is {len(ret_dict['waveform'])}\")\n",
        "        return ret_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform, label = self.load_audio(idx)\n",
        "        # print(f\"in getitem waveform len is {len(waveform)}\")\n",
        "        ret = self.get_dict(waveform, label)\n",
        "        # print(f\"after get dict waveform len is {len(ret['waveform'])}\")\n",
        "        return ret\n",
        "\n",
        "    def labels_list(self):\n",
        "        assert self.CFG.task_type == 'classification', 'wrong task_type, required classification'\n",
        "        return self.intent_encoder.classes_"
      ],
      "metadata": {
        "id": "vSyvQGurJmeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2LDgWnMq8-J"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFyUBGODq-27"
      },
      "outputs": [],
      "source": [
        "def fsc_collate_classifier(inputs):\n",
        "    padded_waveforms = rnn.pad_sequence([data['waveform'] for data in inputs], batch_first=True)\n",
        "    labels = torch.tensor([data['label'] for data in inputs], dtype=torch.long)\n",
        "    padded_text = rnn.pad_sequence([data['encoded_text'] for data in inputs], batch_first=True)\n",
        "    text_lengths = torch.tensor([data['text_length'] for data in inputs], dtype=torch.long)\n",
        "    raw_text = [data['raw_text'] for data in inputs]\n",
        "\n",
        "    return {\n",
        "        'waveform' : padded_waveforms,\n",
        "        'label':labels,\n",
        "        'encoded_text':padded_text,\n",
        "        'text_length':text_lengths,\n",
        "        'raw_text':raw_text\n",
        "    }\n",
        "\n",
        "def asrglue_collate_classifier(inputs):\n",
        "    padded_waveforms = rnn.pad_sequence([data['waveform'] for data in inputs], batch_first=True)\n",
        "    if curCFG.task_type == \"prediction\" :\n",
        "        labels = torch.tensor([data['label'] for data in inputs], dtype=torch.float)\n",
        "    else :\n",
        "        labels = torch.tensor([data['label'] for data in inputs], dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'waveform' : padded_waveforms,\n",
        "        'label':labels,\n",
        "    }\n",
        "\n",
        "def get_dataloaders(CFG, *args, **kwargs):\n",
        "    data_root = CFG.data_root\n",
        "    batch_size = CFG.batch_size \n",
        "    dataset = CFG.dataset\n",
        "    num_workers = CFG.num_of_workers\n",
        "    print(f\"dataset: {dataset}\")\n",
        "\n",
        "    if dataset == 'fsc':\n",
        "        train_dataset = FluentSpeechCommandsDataset(data_root, 'train', *args, **kwargs)\n",
        "        val_dataset = FluentSpeechCommandsDataset(data_root, 'valid', train_dataset.intent_encoder, *args, **kwargs)\n",
        "        test_dataset = FluentSpeechCommandsDataset(data_root, 'test', train_dataset.intent_encoder, *args, **kwargs)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=fsc_collate_classifier, shuffle=True, num_workers=num_workers)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=fsc_collate_classifier, num_workers=num_workers)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=fsc_collate_classifier, num_workers=num_workers)\n",
        "        \n",
        "\n",
        "    elif dataset == 'asrglue':\n",
        "        print(f\"subtask: {CFG.subtask} noise_level: {CFG.noise_level}\")\n",
        "        train_dataset = ASRGLUEDataset(CFG, 'train', *args, **kwargs)\n",
        "        datalen = len(train_dataset)\n",
        "        train_len = int(datalen * 0.8)\n",
        "        \n",
        "        train_dataset, val_dataset = random_split(train_dataset, [train_len, datalen - train_len])\n",
        "       \n",
        "        test_dataset = ASRGLUEDataset(CFG, 'test', *args, **kwargs)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=asrglue_collate_classifier, shuffle=True, drop_last=True, num_workers=num_workers)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=asrglue_collate_classifier, num_workers=num_workers)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=asrglue_collate_classifier, num_workers=num_workers)\n",
        "      \n",
        "    else:\n",
        "        raise ValueError('Invalid dataset, check CFG.dataset')\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf6mORLNfFEI"
      },
      "source": [
        "# Model\n",
        "\n",
        "## Note\n",
        "We use DistilBertTokenizer instead of AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGEcF_hgozc6"
      },
      "outputs": [],
      "source": [
        "class AudioEncoder(torch.nn.Module):\n",
        "      def __init__(\n",
        "          self, \n",
        "          model_name,\n",
        "          trainable = True\n",
        "      ):\n",
        "          super().__init__()\n",
        "          self.model = AutoModel.from_pretrained(model_name)\n",
        "          for p in self.model.parameters():\n",
        "              p.requires_grad = trainable\n",
        "\n",
        "      def forward(self, x):\n",
        "          output = self.model(x)\n",
        "          last_hidden_state = output.last_hidden_state\n",
        "          return last_hidden_state[:,0,:]\n",
        "\n",
        "class E2ESLU(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        CFG: BaseCFG\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        model_name = CFG.audio_encoder_model;\n",
        "        embedding = CFG.audio_embedding\n",
        "        trainable = CFG.trainable\n",
        "        intent_dim = CFG.intent_dim\n",
        "        hidden_dim = CFG.hidden_dim\n",
        "\n",
        "        self.audio_encoder = AudioEncoder(model_name, trainable)\n",
        "\n",
        "        for p in self.audio_encoder.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "        self.final_classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(embedding, hidden_dim),\n",
        "            torch.nn.LeakyReLU(inplace=True),\n",
        "            \n",
        "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
        "            torch.nn.LeakyReLU(inplace=True),\n",
        "            \n",
        "            torch.nn.Linear(hidden_dim, 64),\n",
        "            torch.nn.LeakyReLU(inplace=True),\n",
        "            \n",
        "            torch.nn.Linear(64, intent_dim),\n",
        "           \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.audio_encoder(x)\n",
        "        output = self.final_classifier(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVoW3UT2tKxk"
      },
      "source": [
        "# Train, Valid and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOUy1StMuL7u"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_AzlSbmuKBo"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step, CFG):\n",
        "    device = CFG.device\n",
        "    task_type = CFG.task_type\n",
        "\n",
        "    loss_meter = AverageMeter()\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
        "    # \"\"\"\n",
        "    if task_type == \"prediction\":\n",
        "        loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "    elif task_type == \"classification\":\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "    # \"\"\"\n",
        "\n",
        "    losses = []\n",
        "    train_total = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    for batch in tqdm_object:\n",
        "        print(batch)\n",
        "        # output = model(batch['waveform'].to(device))\n",
        "        output = model(batch['waveform'].to(device=device, dtype=torch.float))\n",
        "        target = batch['label'].to(device)\n",
        "        if task_type == \"prediction\":\n",
        "            output = torch.transpose(output, 0, 1)\n",
        "            target = target.view(1, -1)\n",
        "            pred = output\n",
        "        else:\n",
        "            # classification\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        count = batch['waveform'].size(0)\n",
        "\n",
        "        # print(f\"pred: {pred}\")\n",
        "        # print(f\"target: {target}\")\n",
        "\n",
        "        train_total += count\n",
        "        train_acc += (pred.to(\"cpu\") == target.to(\"cpu\")).sum().item()\n",
        "\n",
        "        loss_meter.update([loss.item()])\n",
        "\n",
        "        tqdm_object.set_postfix(train_loss=loss.item(), train_acc=train_acc/train_total)\n",
        "\n",
        "        losses.append(loss_meter.get())\n",
        "\n",
        "    return loss_meter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valid"
      ],
      "metadata": {
        "id": "YlNZ32LCJJ1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_epoch(model, valid_loader, CFG):\n",
        "    device = CFG.device\n",
        "    task_type = CFG.task_type\n",
        "    \n",
        "    loss_meter = AverageMeter()\n",
        "   \n",
        "    if task_type == \"prediction\":\n",
        "        loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "    else:\n",
        "        # classification\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "   \n",
        "\n",
        "    val_total = 0\n",
        "    val_acc = 0\n",
        "\n",
        "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
        "    for batch in tqdm_object:\n",
        "        output = model(batch['waveform'].to(device=device, dtype=torch.float))\n",
        "        # output = model(batch['waveform'].to(device))\n",
        "        target = batch['label'].to(device)\n",
        "\n",
        "        if task_type == \"prediction\":\n",
        "            output = torch.transpose(output, 0, 1)\n",
        "            target = target.view(1, -1)\n",
        "            pred = output\n",
        "        else:\n",
        "            # classification\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        count = batch[\"waveform\"].size(0)\n",
        "\n",
        "        val_total += count\n",
        "        val_acc += (pred.to(\"cpu\") == target.to(\"cpu\")).sum().item()\n",
        "\n",
        "        loss_meter.update([loss.item()])\n",
        "\n",
        "        tqdm_object.set_postfix(valid_loss=loss_meter.get(), valid_acc=val_acc/val_total)\n",
        "    return loss_meter\n"
      ],
      "metadata": {
        "id": "CSGCO7EoJIm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "uxiJVcbSwzVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, CFG):\n",
        "    pretrain_path = CFG.project_root+\"no_clap_pretrain_best.pt\"\n",
        "    print(f'start test. pretrain_path: {pretrain_path}')\n",
        "    print(f'audio_encoder_lr: {CFG.audio_encoder_lr} downstream_lr: {CFG.downstream_lr}')\n",
        "    \n",
        "    model.load_state_dict(torch.load(pretrain_path))\n",
        "    model.eval()\n",
        "   \n",
        "    test_total = 0\n",
        "    test_acc = 0\n",
        "\n",
        "    tqdm_object = tqdm(test_loader, total=len(test_loader))\n",
        "\n",
        "    if CFG.task_type == \"prediction\":\n",
        "        outputs = torch.tensor([]).to(CFG.device)\n",
        "        targets = torch.tensor([]).to(CFG.device)\n",
        "        for batch in tqdm_object:\n",
        "            output = model(batch['waveform'].to(device=CFG.device, dtype=torch.float))\n",
        "            # output = model(batch['waveform'].to(CFG.device))\n",
        "            target = batch['label'].to(CFG.device)\n",
        "            output = torch.transpose(output, 0, 1)\n",
        "            output = output.view(-1)\n",
        "            #target = target.view(1, -1)\n",
        "\n",
        "            outputs = torch.cat((outputs, output), 0)\n",
        "            targets = torch.cat((targets, target), 0)\n",
        "\n",
        "        print(f\"outputs: {outputs}\")\n",
        "        print(f\"targets: {targets}\")\n",
        "        spearman = SpearmanCorrcoef()\n",
        "        return spearman(outputs, targets).item()\n",
        "    else:\n",
        "        for batch in tqdm_object:\n",
        "            output = model(batch['waveform'].to(device=CFG.device, dtype=torch.float))\n",
        "            # output = model(batch['waveform'].to(CFG.device))\n",
        "            target = batch['label'].to(CFG.device)\n",
        "            \n",
        "            # classification\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "            count = batch[\"waveform\"].size(0)\n",
        "\n",
        "            test_total += count\n",
        "            test_acc += (pred.to(\"cpu\") == target.to(\"cpu\")).sum().item()\n",
        "\n",
        "            tqdm_object.set_postfix(test_acc=test_acc/test_total)\n",
        "\n",
        "    return test_acc/test_total\n"
      ],
      "metadata": {
        "id": "NDVPhLaHww7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5u3uAI7uRyO"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prevent the acc from losting.\n",
        "final_test_acc = 0.0"
      ],
      "metadata": {
        "id": "Vlb52ijKNRFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6LTKxGcuRCY"
      },
      "outputs": [],
      "source": [
        "def train_and_valid_and_test(CFG: BaseCFG, do_train, do_test):\n",
        "    train_loader, valid_loader, test_loader = get_dataloaders(CFG)\n",
        "    print(f\"audio_encoder_model: {CFG.audio_encoder_model}\")\n",
        "    models = E2ESLU(CFG).to(CFG.device, dtype=torch.float)\n",
        "\n",
        "    if do_train:\n",
        "        if CFG.checkpoint_continue:\n",
        "          models.load_state_dict(torch.load(CFG.project_root+\"no_clap_pretrain_best.pt\"))\n",
        "\n",
        "        params = [\n",
        "            {\"params\": models.audio_encoder.parameters(), \"lr\": CFG.audio_encoder_lr},\n",
        "            {\"params\": models.final_classifier.parameters(), \"lr\": CFG.downstream_lr},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            params=params,\n",
        "            weight_decay=CFG.weight_decay,\n",
        "        )\n",
        "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, \n",
        "            mode=\"min\",\n",
        "            patience=CFG.patience,\n",
        "            factor=CFG.factor,\n",
        "        )\n",
        "        step = \"epoch\"\n",
        "\n",
        "        best_loss = float(\"inf\")\n",
        "\n",
        "        for epoch in range(CFG.epochs):\n",
        "            print(f\"Epoch: {epoch+1}\")\n",
        "            models.train()\n",
        "            train_loss = train_epoch(models, train_loader, optimizer, lr_scheduler, step, CFG)\n",
        "            models.eval()\n",
        "            with torch.no_grad():\n",
        "                valid_loss = valid_epoch(models, valid_loader, CFG)\n",
        "\n",
        "            valid_loss_avg = valid_loss.get()\n",
        "            if valid_loss_avg < best_loss:\n",
        "                best_loss = valid_loss_avg\n",
        "                torch.save(models.state_dict(), CFG.project_root+\"no_clap_pretrain_best.pt\")\n",
        "                print(\"Saved Best Model!\")\n",
        "\n",
        "            if (step == \"epoch\"):\n",
        "                lr_scheduler.step(valid_loss_avg)\n",
        "    if do_test:\n",
        "        with torch.no_grad():\n",
        "            final_test_acc = test(models, test_loader, CFG)\n",
        "            print(f\"Finish! Result: test_acc: {final_test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_and_valid_and_test(curCFG, do_train, do_test)"
      ],
      "metadata": {
        "id": "KqWJRC4DCwtd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Comparison_of_Encoders_on_SLU_tasks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}